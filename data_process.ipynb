{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 处理问题\n",
    "    更具单词便 token 一下，然后变成最长长度。\n",
    "# 处理图像特征\n",
    "    转化为hd5f 文件\n",
    "# 处理context \n",
    "    fasttext, 编码每一个context\n",
    "    补全到最大context的内容\n",
    "    采用预处理的方法，先将图片中的context都处理一下，处理完了保存为hd5f 文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from fasttext import load_model\n",
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "SENTENCE_SPLIT_REGEX = re.compile(r\"(\\W+)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 文本处理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除单词中逗号，问好， 's 加上空格\n",
    "def word_tokenize(word):\n",
    "    word = word.lower()\n",
    "    word = word.replace(\",\", \"\").replace(\"?\", \"\").replace(\"'s\", \" 's\") \n",
    "    return word.strip()\n",
    "\n",
    "# 处理句子\n",
    "def tokenize(sentence, regex=SENTENCE_SPLIT_REGEX, keep=[\"'s\"], remove=[\",\", \"?\"]):\n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    for token in keep:\n",
    "        sentence = sentence.replace(token, \" \" + token)\n",
    "\n",
    "    for token in remove:\n",
    "        sentence = sentence.replace(token, \"\")\n",
    "\n",
    "    tokens = regex.split(sentence)\n",
    "    tokens = [t.strip() for t in tokens if len(t.strip()) > 0]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fasttext 处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载fasttext 模型\n",
    "def load_fasttext_model():\n",
    "    fasttext_model_file_path = os.path.join('../pythia', \".vector_cache\", \"wiki.en.bin\")\n",
    "    fasttext_model = load_model(fasttext_model_file_path)\n",
    "    return fasttext_model\n",
    "\n",
    "def wordToVector(context):\n",
    "    fasttext_model = load_fasttext_model()\n",
    "    if len(context)==1:\n",
    "        return np.mean([fasttext_model.get_word_vector(w) for w in context.split(\" \")], axis=0)\n",
    "    else:\n",
    "        w_embedding = np.zeros((len(context), 300))\n",
    "        for i in range(len(context)):\n",
    "            w_embedding[i] = np.mean([fasttext_model.get_word_vector(w) for w in context[i].split(\" \")], axis=0)\n",
    "        return w_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 处理 ocr 文本，将文本存储为hdf5文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "(1) 获得每个图片的context\n",
    "(2) 将所有的context，使用fasttext 编码\n",
    "(3) 编码完了，保存所有的fasttext 编码到一个hdf5 文件中\n",
    "    1. 保存特征文件\n",
    "    2. 保存image id 和 context index 的map 文件\n",
    "文件路径：\n",
    "\n",
    "'''\n",
    "data_dir = {\n",
    "    'test' :'../data/imdb/textvqa_0.5/imdb_textvqa_test.npy', \n",
    "    'train':'../data/imdb/textvqa_0.5/imdb_textvqa_train.npy',\n",
    "    'val'  : '../data/imdb/textvqa_0.5/imdb_textvqa_val.npy'}\n",
    "\n",
    "# 统计所有的图片id，和总数量\n",
    "def statistic_image_id():\n",
    "    imageid_2_ocrnum = {}\n",
    "    ocr_statistic = {}\n",
    "    for key in data_dir.keys():\n",
    "        data = np.load(open(data_dir[key],'rb'), allow_pickle=True)\n",
    "        for i in range(1,data.size):\n",
    "            if imageid_2_ocrnum.get(data[i]['image_id'], None) == None:\n",
    "                imageid_2_ocrnum[data[i]['image_id']] = len(data[i]['ocr_tokens'])\n",
    "                ocr_statistic[len(data[i]['ocr_tokens'])] = ocr_statistic.get(len(data[i]['ocr_tokens']),0) + 1\n",
    "    return imageid_2_ocrnum, len(imageid_2_ocrnum), ocr_statistic\n",
    "\n",
    "def ContextToVectorByFasttext(save = False):\n",
    "    # load fasttext model \n",
    "    fasttext_model = load_fasttext_model()\n",
    "    # 目录\n",
    "    context_feature_hdf5 = '../data/imdb/textvqa_0.5/context_embeddding.hdf5'\n",
    "    context_imageid2index = \"../data/imdb/textvqa_0.5/context_imageid2index.pkl\"\n",
    "    imageid2contextnum = \"../data/imdb/textvqa_0.5/imageid2contextnum.pkl\"\n",
    "    context_file = h5py.File(context_feature_hdf5, \"w\")\n",
    "    # 统计所有图片id\n",
    "    imageid_2_ocrnum, num, ocr_num = statistic_image_id()\n",
    "    print(\"总数：\", num)\n",
    "    max_num = 50\n",
    "    em_dim = 300\n",
    "    # create context embedding file\n",
    "    imageid_2_contextindex = {}\n",
    "    context_embedding = context_file.create_dataset('context_embedding', (sum(imageid_2_ocrnum.values()), em_dim), 'f')\n",
    "    \n",
    "    num_without_ocr = 0\n",
    "    num_image = 0\n",
    "    index = 0\n",
    "    for key in data_dir.keys():\n",
    "        # loda textvqa data\n",
    "        data = np.load(open(data_dir[key],'rb'), allow_pickle=True)\n",
    "        for i in range(1,data.size):\n",
    "            ocr_tokens = data[i]['ocr_tokens']\n",
    "            image_id = data[i]['image_id']\n",
    "            \n",
    "    #         ocr_info = data[i]['ocr_info']  文本框的信息后面再处理\n",
    "            if imageid_2_contextindex.get(image_id,None) == None:\n",
    "                num_image += 1\n",
    "                \n",
    "                imageid_2_contextindex[image_id] = index\n",
    "                \n",
    "                if len(ocr_tokens)==0:\n",
    "                    num_without_ocr += 1\n",
    "                \n",
    "                for j in range(len(ocr_tokens)):\n",
    "                    word = ocr_tokens[j]\n",
    "                    words_embedding = np.mean([fasttext_model.get_word_vector(w) for w in word_tokenize(word).split(\" \")], axis=0)\n",
    "                    context_embedding[index, :] = words_embedding # ,100,2048\n",
    "                    index += 1\n",
    "                \n",
    "    context_file.close()\n",
    "    pickle.dump(imageid_2_contextindex, open(context_imageid2index, 'wb'))\n",
    "    pickle.dump(imageid_2_ocrnum, open(imageid2contextnum, 'wb'))\n",
    "    print(\"没有ocr的图片个数：%d, 总共%d\"%(num_without_ocr, num_image))\n",
    "\n",
    "# save data\n",
    "# ContextToVectorByFasttext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read ocr 文件 according image_id\n",
    "data_dir = '../data/imdb/textvqa_0.5/'\n",
    "context_feature_file = data_dir + \"context_embeddding.hdf5\"\n",
    "context_imageid2index = data_dir + \"context_imageid2index.pkl\"\n",
    "imageid2contextnum = data_dir + \"imageid2contextnum.pkl\"\n",
    "\n",
    "c_imageid_i = pickle.load(open(context_imageid2index,'rb'))\n",
    "c_imageid_num = pickle.load(open(imageid2contextnum,'rb'))\n",
    "with h5py.File(context_feature_file, 'r') as hf:\n",
    "    c_features = np.array(hf.get('context_embedding'))\n",
    "image_id = val_data[1]['image_id']\n",
    "feature = c_features[c_imageid_i[image_id]:c_imageid_i[image_id]+c_imageid_num[image_id]]  # ocr 相互联系起来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = np.load(open(data_dir['val'],'rb'), allow_pickle=True)\n",
    "# d.get_question_sequence(val_data[1]['question_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'creation_time': 1550905257.4633052,\n",
       " 'version': 0.5,\n",
       " 'dataset_type': 'val',\n",
       " 'has_answer': True}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 产生每个内容对应的顺序\n",
    "order_vectors = torch.eye(10)\n",
    "order_vectors[context[\"length\"] :] = 0\n",
    "sample.order_vectors = order_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = min(len(tokens), self.max_length)\n",
    "tokens = tokens[:length]\n",
    "\n",
    "output = torch.full(\n",
    "            (self.max_length, self.model.get_dimension()),\n",
    "            fill_value=self.PAD_INDEX,\n",
    "            dtype=torch.float,\n",
    "        )\n",
    "\n",
    "for idx, token in enumerate(tokens):\n",
    "    output[idx] = torch.from_numpy(self.stov[token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import TextVQA, Dictionary\n",
    "from basemodel import build_model\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from torch.autograd import Variable\n",
    "import yaml\n",
    "import torch.nn as nn\n",
    "import os\n",
    "\n",
    "d = Dictionary()\n",
    "val_data = TextVQA('val', d)\n",
    "train_data = TextVQA('train', d)\n",
    "\n",
    "weight = d.create_glove_embedding_init(pre=True,pre_dir='../data/vocabs/embedding_weight.npy')\n",
    "with open('options/9/29/9_29_3.yml', 'r') as handle:\n",
    "    config = yaml.load(handle, Loader=yaml.FullLoader)\n",
    "\n",
    "model = build_model(val_data, config['model_attributes'])\n",
    "# model = nn.DataParallel(model)\n",
    "# model.cuda()\n",
    "val_loader = DataLoader(ConcatDataset([train_data, val_data]), 1, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = False\n",
    "for i, sample in enumerate(val_loader):\n",
    "    question = Variable(sample[\"question\"])\n",
    "    img = Variable(sample[\"img_feature\"])\n",
    "    context = Variable(sample[\"context_feature\"])\n",
    "    context_order = Variable(sample[\"order_vectors\"])\n",
    "    label = Variable(sample['answer'])\n",
    "    \n",
    "    if gpu:\n",
    "        question = question.cuda()\n",
    "        img = img.cuda()\n",
    "        context = context.cuda()\n",
    "        context_order = context_order.cuda()\n",
    "        label = label.cuda()\n",
    "    \n",
    "    score = model(img, question, context, context_order)#.cuda()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_source = 0\n",
    "answer_index = []\n",
    "question_id = []\n",
    "for i in range(len(val_data)):\n",
    "    if val_data.entries[i][\"answer_source\"] == {}:\n",
    "        answer_source += 1\n",
    "        answer_index.append(i)\n",
    "        question_id.append(val_data.entries[i][\"question_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({2: 568,\n",
       "         3: 1033,\n",
       "         0: 1072,\n",
       "         1: 1963,\n",
       "         7: 48,\n",
       "         5: 176,\n",
       "         4: 88,\n",
       "         6: 19,\n",
       "         9: 14,\n",
       "         11: 5,\n",
       "         12: 3,\n",
       "         23: 1,\n",
       "         17: 1,\n",
       "         14: 1,\n",
       "         16: 2,\n",
       "         8: 2,\n",
       "         19: 2,\n",
       "         32: 1,\n",
       "         15: 1})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "counter = Counter()\n",
    "counter.update(answer_source)\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(1,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False, False, False, False, False, False, False]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.new_ones(1,10).eq(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "sigmoid(input) -> Tensor\n",
       "\n",
       "Applies the element-wise function :math:`\\text{Sigmoid}(x) = \\frac{1}{1 + \\exp(-x)}`\n",
       "\n",
       "See :class:`~torch.nn.Sigmoid` for more details.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/anaconda2/envs/py3_pt4/lib/python3.7/site-packages/torch/nn/functional.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def _mask_attentions(attention, image_locs):\n",
    "    batch_size, num_loc, n_att = attention.size()\n",
    "    tmp1 = attention.new_zeros(num_loc)\n",
    "    tmp1[:num_loc] = torch.arange(0, num_loc, dtype=attention.dtype).unsqueeze(\n",
    "        dim=0\n",
    "    )\n",
    "\n",
    "    tmp1 = tmp1.expand(batch_size, num_loc)\n",
    "    tmp2 = image_locs.type(tmp1.type())\n",
    "    tmp2 = tmp2.unsqueeze(dim=1).expand(batch_size, num_loc)\n",
    "    mask = torch.ge(tmp1, tmp2)\n",
    "    mask = mask.unsqueeze(dim=2).expand_as(attention)\n",
    "    attention = attention.masked_fill(mask, 0)\n",
    "    return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 2048])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_att_feature.type(text_encode.type()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils import compute_result\n",
    "# model_dir = os.path.join(\"save/9/exp_9_17_1/\", \"model_best.pth\")\n",
    "# compute_result(\"val\", model, model_dir , val_loader, \"save/9/exp_9_17_1/\", 10, gpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(open(\"../data/imdb/textvqa_0.5/imdb_textvqa_train.npy\",\"rb\"), allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "a = [{\"12\":\"34\"}]\n",
    "path_rslt = os.path.join(\"./save/\", \"test.json\")\n",
    "with open(path_rslt, 'w') as handle:\n",
    "    json.dump(a, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_attention = Channel_attention(dim_x = 100, dim_y=200, h_dim=1280)\n",
    "x = torch.rand(10,20,100)\n",
    "y = torch.rand(10,30,200)\n",
    "x1, y1 = channel_attention(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ContextEmbedding(300,1280,1280,1,[0.2,0],0.2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.rand(2,50,300)\n",
    "mask = torch.rand(2,50) == torch.rand(2,50)\n",
    "q = torch.rand(2,1280)\n",
    "o = torch.rand(2,50,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 图像和ocr文本之间进行交互"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from attention import DenseCoAttn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = DenseCoAttn(1024, 1024, 2, 0, 2, 0.0)\n",
    "x = torch.randn(128, 100, 1024)\n",
    "x_m = torch.zeros(128, 100).float()\n",
    "x_m[:,80] = 1\n",
    "y = torch.randn(128, 50, 1024)\n",
    "y_m = torch.zeros(128, 50).float()\n",
    "y_m[:,40] = 1\n",
    "weighted1, weighted2 = net(x,y,x_m,y_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3_pt4]",
   "language": "python",
   "name": "conda-env-py3_pt4-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
