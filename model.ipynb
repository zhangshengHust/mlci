{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import yaml\n",
    "from layers import SimpleClassifier, WordEmbedding, VisualFeatEncoder, QuestionEmbedding\n",
    "import torch.nn.functional as F\n",
    "from dfaf import SingleBlock, Classifier\n",
    "from torch.nn.utils.weight_norm import weight_norm\n",
    "from basemodel import build_model\n",
    "\n",
    "config = yaml.load(open('options/1/exp_1_8_3.yaml','rb'),Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRRA(nn.Module):\n",
    "    def __init__(self, \n",
    "                 embedding, \n",
    "                 qembedding, \n",
    "                 image_encoder,\n",
    "                 interaction_net, \n",
    "                 classifier, \n",
    "                 **arg):\n",
    "        super().__init__()\n",
    "        self.embedding = embedding\n",
    "        self.qembedding = qembedding\n",
    "        self.image_encoder = image_encoder\n",
    "        \n",
    "        self.interaction_net = interaction_net\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def forward(self, image, bbox, text, context):\n",
    "        \n",
    "        '''\n",
    "        image:     (b, N, 2048)\n",
    "        bbox:      (b, N, 4)\n",
    "        text :     (b, T)\n",
    "        contex :   (b, M, 300)\n",
    "        '''\n",
    "        text_embedding = self.embedding(text)                        # we sequence\n",
    "        text_encode ,_ = self.qembedding.forward_all(text_embedding)\n",
    "        image_encode = self.image_encoder(image, bbox)\n",
    "        \n",
    "        '''\n",
    "        obtain the mask of image , question and context feature\n",
    "        '''\n",
    "        text_mask = (self.get_mask(text_embedding) == False).float()\n",
    "        context_mask = (self.get_mask(context) == False).float()\n",
    "        image_mask = (self.get_mask(image_encode) == False).float()\n",
    "        \n",
    "        v, q, c = self.interaction_net(image_encode, text_encode, context, image_mask, text_mask, context_mask)\n",
    "        print(v.shape, q.shape, c.shape)\n",
    "        score1 = self.classifier[0](v, q, image_mask, text_mask)\n",
    "        score2 = self.classifier[1](c).squeeze(dim=2)\n",
    "        score = torch.cat((score1,score2), dim = 1)\n",
    "        return score\n",
    "    \n",
    "    def get_mask(self, x):\n",
    "        return (x.abs().sum(dim=2) == 0)\n",
    "    \n",
    "def build_model(dataset, config):\n",
    "    \n",
    "    embedding = WordEmbedding(\n",
    "        len(dataset.dictionary), \n",
    "        dataset.dictionary.embedding_dim\n",
    "    )\n",
    "    \n",
    "    qembedding = QuestionEmbedding(**config['text_embeddings'])\n",
    "    \n",
    "    image_encoder = VisualFeatEncoder(\n",
    "            in_dim=config['image_feature_dim'],\n",
    "            **config['image_feature_encodings']\n",
    "    )\n",
    "    \n",
    "    interaction_net = SingleBlock(**config['interIntrablocks'])\n",
    "    config[\"classifier\"][\"out_features\"] = dataset.answer_process.length\n",
    "    classifier1 = Classifier(**config[\"classifier\"])\n",
    "    classifier2 = weight_norm(nn.Linear(512, 1), dim=None)\n",
    "    classifier = nn.ModuleList([classifier1, classifier2])\n",
    "    \n",
    "    modules = {\n",
    "        'embedding': embedding,\n",
    "        'qembedding': qembedding,\n",
    "        'image_encoder': image_encoder,\n",
    "        \"interaction_net\": interaction_net,\n",
    "        'classifier' : classifier\n",
    "    }\n",
    "\n",
    "    return LoRRA(**modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 5000 val samples.\n",
      "Use 5000 val samples.\n",
      "no existing answer 1345\n"
     ]
    }
   ],
   "source": [
    "from dataset import Dictionary, TextVQA\n",
    "from torch.utils.data import DataLoader\n",
    "dictionary = Dictionary()\n",
    "embedding_weight = dictionary.create_glove_embedding_init(pre=True, pre_dir='../data/vocabs/embedding_weight.npy')\n",
    "# train_dset = TextVQA('train', dictionary)\n",
    "eval_dset = TextVQA('val', dictionary)\n",
    "eval_loader = DataLoader(\n",
    "            eval_dset, \n",
    "            2, \n",
    "            shuffle=False, \n",
    "            num_workers = 1, \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(eval_dset, config[\"model_attributes\"])\n",
    "model = nn.DataParallel(model).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([nan, nan, nan], device='cuda:0', grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# F.softmax(model.alpha,dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_parameter = torch.load(\"save/12/exp_1_8_3/model_best.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(model_parameter.get('model_state', model_parameter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4960, 0.3180, 0.1860], device='cuda:0', grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(model.module.alpha,dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adamax(\n",
    "        filter(lambda p:p.requires_grad, model.parameters()),\n",
    "        lr = 0.0015\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 完成一次forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from utils import LogitBinaryCrossEntropy\n",
    "lbce = LogitBinaryCrossEntropy()\n",
    "for i,sample in enumerate(eval_loader):\n",
    "    input_ids = Variable(sample[\"input_ids\"])\n",
    "    token_type_ids = Variable(sample[\"token_type_ids\"])\n",
    "    attention_mask = Variable(sample[\"attention_mask\"])\n",
    "    img = Variable(sample[\"img_feature\"])\n",
    "    context = Variable(sample[\"context_feature\"])\n",
    "    labels = Variable(sample['answer'])\n",
    "    bbox = Variable(sample['bbox'])\n",
    "    ocrbbox = Variable(sample['ocrbbox'])\n",
    "    answer = model(img.cuda(), bbox.cuda(), input_ids.cuda(), token_type_ids.cuda(), attention_mask.cuda(), context.cuda(), ocrbbox.cuda())  # image, bbox, text, context, cbbox\n",
    "    loss = lbce(labels.cuda(), answer)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = nn.Parameter(torch.ones(10,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQAAccuracy(object):\n",
    "    \"\"\"\n",
    "    Calculate VQAAccuracy. Find more information here_\n",
    "\n",
    "    **Key**: ``vqa_accuracy``.\n",
    "\n",
    "    .. _here: https://visualqa.org/evaluation.html\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(VQAAccuracy,self).__init__()\n",
    "\n",
    "    def _masked_unk_softmax(self, x, dim, mask_idx):\n",
    "        x1 = torch.nn.functional.softmax(x, dim=dim)\n",
    "        x1[:, mask_idx] = 0\n",
    "        x1_sum = torch.sum(x1, dim=1, keepdim=True)\n",
    "        y = x1 / x1_sum\n",
    "        return y\n",
    "\n",
    "    def calculate(self, expected, output, *args, **kwargs):\n",
    "        \"\"\"Calculate vqa accuracy and return it back.\n",
    "        Args:\n",
    "            output : score\n",
    "            expected : label\n",
    "        Returns:\n",
    "            torch.FloatTensor: VQA Accuracy\n",
    "\n",
    "        \"\"\"\n",
    "        output = self._masked_unk_softmax(output, 1, 0) # unknow 屏蔽掉了\n",
    "        output = output.argmax(dim=1)  # argmax\n",
    "\n",
    "        one_hots = expected.new_zeros(*expected.size())\n",
    "        print(\"one hot:\", one_hots,\"output:\", output)\n",
    "        one_hots.scatter_(1, output.view(-1, 1), 1)\n",
    "        print(one_hots)\n",
    "        scores = one_hots * expected\n",
    "        accuracy = torch.sum(scores) / expected.size(0)\n",
    "\n",
    "        return accuracy\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.calculate(*args, **kwargs)\n",
    "vqa_accuracy = VQAAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.4101,  0.6817,  1.1464,  1.4361,  0.1126, -0.3576,  0.1192,  0.0676,\n",
       "           2.2384, -1.7476]]),\n",
       " tensor([[0., 0., 0., 1., 1., 0., 0., 1., 1., 1.]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "output = torch.randn(1,10)\n",
    "labels = (torch.randn(1,10)>0).float()\n",
    "output, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### trilinear interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trilinear import TriAttention, TCNet\n",
    "from fc import FCNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "v = torch.randn(2,50,789)\n",
    "q = torch.randn(2,12,789)\n",
    "a = torch.randn(2,5,789)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1024])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Trilinear_Classifier(nn.Sequential):\n",
    "    def __init__(self, in_features, mid_features, out_features, drop=0.0):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lin1 = nn.Linear(in_features, mid_features)\n",
    "        self.lin2 = nn.Linear(mid_features, out_features)\n",
    "        self.bn = nn.BatchNorm1d(mid_features)\n",
    "        #_____________________________________________________________________________________\n",
    "        v_dim = 768               \n",
    "        q_dim = 768\n",
    "        a_dim = 768               \n",
    "        h_mm = 768               \n",
    "        rank = 32               \n",
    "        gamma = 1               \n",
    "        k = 1               \n",
    "        h_out = 1               \n",
    "        t_att = TriAttention(v_dim, q_dim, a_dim, h_mm, 1, rank, gamma, k, dropout=[.2, .5])               \n",
    "        t_net = TCNet(v_dim, q_dim, a_dim, h_mm, h_out, rank, 1, dropout=[.2, .5], k=1)               \n",
    "        #______________________________________________________________________________________\n",
    "\n",
    "    def forward(self, v, q, c, v_mask, q_mask):\n",
    "        \"\"\"\n",
    "        v: visual feature      [batch, num_obj, 512]\n",
    "        q: question            [batch, max_len, 512]\n",
    "        v_mask                 [batch, num_obj]\n",
    "        q_mask                 [batch, max_len]\n",
    "        \"\"\"\n",
    "        att, logits = t_att(v, q, c)  # b x v x q x a x g\n",
    "        fusion_f = t_net.forward_with_weights(v, q, c, att[:, :, :, :, 0])\n",
    "        out = self.lin1(self.drop(fusion_f))\n",
    "        out = self.lin2(self.drop(self.relu(self.bn(out))))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 1.]]), tensor([[0.8401, 0.4474]]), tensor([0.5000, 0.5000]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "lable = torch.Tensor([[0,1]])\n",
    "prediction = torch.rand(1,2)\n",
    "weight = torch.Tensor([0.5, 0.5])\n",
    "lable, prediction, weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8466)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.binary_cross_entropy_with_logits(prediction, lable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4233)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.binary_cross_entropy_with_logits(prediction, lable, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlevelr = []\n",
    "for l in range(3):\n",
    "    v = torch.randn(1, 100, 512)\n",
    "    q = torch.randn(1, 14, 512)\n",
    "    c = torch.randn(1, 50, 512)\n",
    "    mlevelr.append((v,q,c))\n",
    "mask_q = (torch.randn(1,100)>0.5).float()\n",
    "mask_v = (torch.randn(1,14)>0.5).float()\n",
    "mask_c = (torch.randn(1,50)>0.5).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "atten = m(mlevelr, mask_q, mask_v, mask_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 1])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atten.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3_pt4]",
   "language": "python",
   "name": "conda-env-py3_pt4-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
